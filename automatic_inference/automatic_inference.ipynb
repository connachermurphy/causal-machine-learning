{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *In progress*: Automatic Inference\n",
    "I am in the process of repurposing the code written for the final exam of the course discussed below. The original code is commented and will slowly be incorporated into a (hopefully) coherent codebase.\n",
    "\n",
    "Author: Connacher Murphy\n",
    "\n",
    "I implement the procedure described in [Farrell, Liang, and Misra (2021)](https://arxiv.org/abs/2010.14694). I make use of R code provided in the Causal Machine Learning course offered in the Fall of 2023 by Max Farrell and Sanjog Misra.\n",
    "\n",
    "The parameter of interest is $\\mu_0 = \\mathbb{E}[\\mathbf{H}(\\mathbf{X},\\mathbf{\\theta}(\\mathbf{X}); \\mathbf{Z})]$. The outcome variable $Y$ is linked to the parameter functions $\\mathbf{\\theta}(\\cdot)$ by the equality $\\mathbb{E}[Y | \\mathbf{X} = \\mathbf{x}, \\mathbf{Z} = \\mathbf{z}] = G(\\mathbf{\\theta}(\\mathbf{X}), \\mathbf{Z})$.\n",
    "\n",
    "When projecting the Hessian of the loss function onto $\\mathbf{X}$ for the estimation of $\\mathbf{\\Lambda}(\\mathbf{X})$, it is sometimes possible to avoid estimation. For example, with a linear $G(\\mathbf{\\theta}(\\mathbf{X}), \\mathbf{Z})$ and squared loss, we can compute the Hessian directly. This code does _not_ account for such possibilities and will rely on automatic differentiation for the Hessian and a DNN for the projection of this Hessian onto X\n",
    "\n",
    "_Caution_: Some parts of this code are specialized to the $\\operatorname{dim}(\\mathbf{\\theta}) = 2$ case. I plan to make the code more flexible along this dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import automatic_inference as auto_inf\n",
    "\n",
    "# import numpy as np\n",
    "# import torch.linalg as linalg\n",
    "# import torch.autograd as autograd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data generating process (DGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)  # Set the seed for reproducibility\n",
    "\n",
    "N = 15000  # observation count\n",
    "K = 2  # feature count\n",
    "\n",
    "# Draw independent features from standard normal\n",
    "dnn_features = torch.randn(N, K)\n",
    "\n",
    "# Build the structural parameters from features\n",
    "structural_parameters = torch.cat(\n",
    "    (dnn_features[:, 0].view(N, 1), 3 + dnn_features[:, 1].view(N, 1)), dim=1\n",
    ")\n",
    "structural_parameters_dim = structural_parameters.shape[1]\n",
    "\n",
    "# The structural feature is a binary treatment indicator\n",
    "structural_features = 1 * (torch.randn(N, 1) > 0).view(N, 1)\n",
    "\n",
    "\n",
    "# Define the correspondence between structural parameters and structural features\n",
    "# We use a linear correspondence here; let's not get too crazy\n",
    "# CM: this is a pretty common structural layer, so I should move it to the .py file\n",
    "def structural_layer(structural_parameters, structural_features):\n",
    "    structural_layer_eval = structural_parameters[:, 0:1] + torch.sum(\n",
    "        (structural_features * structural_parameters[:, 1:]), axis=1, keepdim=True\n",
    "    )\n",
    "\n",
    "    return structural_layer_eval\n",
    "\n",
    "\n",
    "# Calculate outcomes (structural component + noise)\n",
    "outcomes_structural = structural_layer(structural_parameters, structural_features)\n",
    "outcomes = outcomes_structural + torch.randn(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CM: create a function for sample splits\n",
    "# CM: check N / splits != integer case\n",
    "\n",
    "# Create splits\n",
    "perm = torch.randperm(N)  # create a permutation of the indices\n",
    "\n",
    "num_splits = 3  # number of splits\n",
    "\n",
    "split_size = N // num_splits  # compute the size of each split\n",
    "\n",
    "splits = []  # store splits in a list of dictionaries\n",
    "for s in range(num_splits):\n",
    "    indices = perm[s * split_size : (s + 1) * split_size]\n",
    "\n",
    "    # Use indices to create a split\n",
    "    split = {\n",
    "        \"dnn_features\": dnn_features[indices],\n",
    "        \"structural_features\": structural_features[indices],\n",
    "        \"outcomes\": outcomes[indices],\n",
    "        \"structural_parameters\": structural_parameters[indices],\n",
    "    }\n",
    "\n",
    "    # Add the split to the list of splits\n",
    "    splits.append(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Estimate structural parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_sizes = [30, 30]\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 0.0  # no L2 regularization\n",
    "num_epochs = 2000\n",
    "\n",
    "# Initialize loss function; we use mean squared error\n",
    "loss_function = nn.MSELoss(reduction = \"mean\")\n",
    "\n",
    "# We will initialize the model and optimizer in each loop below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating theta\n",
      "Split 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 749.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 691.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 758.21it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimating structural parameters\")\n",
    "\n",
    "model_structural_parameters = []  # trained models\n",
    "\n",
    "for s in range(num_splits):\n",
    "    print(f\"Split {s + 1}\")\n",
    "\n",
    "    # Initialize neural network\n",
    "    model = auto_inf.DeepNeuralNetworkReLU(\n",
    "        input_dim=K,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        output_dim=structural_parameters_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer; we use stochastic gradient descent\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model_fit = auto_inf.train_dnn(\n",
    "        splits[s][\"dnn_features\"],\n",
    "        splits[s][\"structural_features\"],\n",
    "        splits[s][\"outcomes\"],\n",
    "        structural_layer,\n",
    "        model,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        num_epochs,\n",
    "        noisily=False,\n",
    "    )\n",
    "\n",
    "    model_structural_parameters.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CM: add diagnostics for structural parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Estimate $\\mathbf{\\Lambda}(\\mathbf{X})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CM: up next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Estimate $\\mathbf{\\Lambda}(\\mathbf{X})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def identity(theta, Z): # simple identity function for G(Theta)\n",
    "#     return(theta)\n",
    "\n",
    "# # Estimate Lambda for a given split and DNN\n",
    "# def estimate_Lambda(split, dnn, theta_dim, G, loss_function):\n",
    "#     print(f'Split {split + 1} with DNN {dnn + 1}')\n",
    "\n",
    "#     # Number of observations in the split\n",
    "#     N = splits[split]['X'].size(0)\n",
    "\n",
    "#     # Evaluate the structural parameter functions\n",
    "#     theta = model_theta[dnn](splits[split]['X'])\n",
    "\n",
    "#     # Predict outcomes\n",
    "#     outputs = G(theta, splits[split]['Z'])\n",
    "\n",
    "#     # Calculate loss\n",
    "#     loss = loss_function(outputs, splits[split]['Y'])\n",
    "\n",
    "#     # Gradient of loss w.r.t. theta\n",
    "#     theta_grad = autograd.grad(\n",
    "#         loss, theta, create_graph = True, retain_graph= True\n",
    "#     )[0]\n",
    "\n",
    "#     # Initialize Hessian\n",
    "#     Lambda = torch.zeros([N, theta_dim, theta_dim])\n",
    "    \n",
    "#     for k in range(theta_dim):\n",
    "#         hess_row = autograd.grad( # row k of Hessian\n",
    "#             theta_grad[:,k].sum(), theta, retain_graph = True\n",
    "#         )\n",
    "\n",
    "#         for j in range(k, theta_dim): # only need to compute upper triangle\n",
    "#             print(f'Element ({k}, {j})')\n",
    "            \n",
    "#             # Extract Hessian element (k,j)\n",
    "#             hess_element = hess_row[0][:,j]\n",
    "\n",
    "#             # Project Hessian element (k,j) onto X\n",
    "#             hess_element_projection = train_DNN( \n",
    "#                 splits[split]['X'], hess_element.view(N, 1), splits[split]['Z'],\n",
    "#                 1, identity, [30, 30], 0.0, nn.MSELoss(reduction = 'mean'),\n",
    "#                 learning_rate, weight_decay, num_epochs\n",
    "#             )\n",
    "\n",
    "#             # Store projection\n",
    "#             Lambda[:,k,j] = hess_element_projection(splits[split]['X']).view(N)\n",
    "\n",
    "#             if k != j: # reflect upper triangle to lower triangle\n",
    "#                 Lambda[:,j,k] = Lambda[:,k,j]\n",
    "#                 print(f'Reflecting to element ({j}, {k})')\n",
    "#     print('\\n')\n",
    "\n",
    "#     return(Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "# print('Projecting Hessian onto X\\n')\n",
    "\n",
    "# Lambdas = []\n",
    "\n",
    "# Lambdas.append(estimate_Lambda(0, 2, theta_dim, G, loss_function))\n",
    "# Lambdas.append(estimate_Lambda(1, 0, theta_dim, G, loss_function))\n",
    "# Lambdas.append(estimate_Lambda(2, 1, theta_dim, G, loss_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Influence function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estimate influence function for a given split, DNN, and Lambda\n",
    "# def estimate_influence_function(split, dnn, Lambda, theta_dim, G, H, loss_function):\n",
    "#     print(f'Split {split + 1} with DNN {dnn + 1}')\n",
    "    \n",
    "#     # Number of observations in the split\n",
    "#     N = splits[split]['X'].size(0)\n",
    "\n",
    "#     # Evaluate the structural parameter functions\n",
    "#     theta = model_theta[dnn](splits[split]['X'])\n",
    "    \n",
    "#     # Predict outcomes\n",
    "#     outputs = G(theta, splits[split]['Z'])\n",
    "\n",
    "#     # Calculate loss\n",
    "#     loss = loss_function(outputs, splits[split]['Y'])\n",
    "\n",
    "#     # Gradient of loss w.r.t. theta\n",
    "#     theta_grad = autograd.grad(\n",
    "#         loss, theta, create_graph = True, retain_graph= True\n",
    "#     )[0]\n",
    "\n",
    "#     # Evaluate H(.)\n",
    "#     H_eval = H(theta, splits[split]['Z'])\n",
    "\n",
    "#     # Gradient of H for adjustment term\n",
    "#     H_theta = autograd.grad(H_eval.sum(), theta)[0]\n",
    "\n",
    "#     influence_function = H_eval - torch.matmul(\n",
    "#         torch.matmul(\n",
    "#             H_theta.view(N, 1, theta_dim), linalg.pinv(Lambdas[Lambda])\n",
    "#         ).view(N, 1, theta_dim),\n",
    "#         theta_grad.view(N, theta_dim, 1)\n",
    "#     ).view(N, 1)\n",
    "\n",
    "#     return(influence_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define H(theta) function\n",
    "# def H(theta, Z): # we let H(theta(X)) = beta(X)\n",
    "#     N = Z.size(0)\n",
    "#     return(theta[:,1].view(N, 1))\n",
    "\n",
    "# loss_function = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "# print('Estimating influence function')\n",
    "\n",
    "# influence_function = [] # store influence function estimates\n",
    "\n",
    "# influence_function.append(\n",
    "#     estimate_influence_function(0, 1, 2, theta_dim, G, H, loss_function)\n",
    "# )\n",
    "# influence_function.append(\n",
    "#     estimate_influence_function(1, 2, 0, theta_dim, G, H, loss_function)\n",
    "# )\n",
    "# influence_function.append(\n",
    "#     estimate_influence_function(2, 0, 1, theta_dim, G, H, loss_function)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate influence function values across splits and store as np array\n",
    "# influence_function_np = np.concatenate([\n",
    "#     influence_function[0].detach().numpy(),\n",
    "#     influence_function[1].detach().numpy(),\n",
    "#     influence_function[2].detach().numpy()\n",
    "# ])\n",
    "\n",
    "# # Calculate estimate and standard error from concatenated influence function\n",
    "# est = influence_function_np.mean()\n",
    "# se = math.sqrt(influence_function_np.var() / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Report results\n",
    "# print('Mean:', round(est, 4))\n",
    "# print('S.E.:', round(se, 4))\n",
    "# print('95% CI: [', round(est - 1.96 * se, 4), ', ',\n",
    "#       round(est + 1.96 * se, 4), ']', sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-machine-learning-2AFA5HUo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
